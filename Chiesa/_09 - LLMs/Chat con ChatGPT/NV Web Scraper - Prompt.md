---
creato: 2025/08/04 06:49:27
modificato: 2025/08/04 06:49:27
---

- Modifico il codice in Visual Studio Code aggiornato
- Ho Python 3.13.5 (latest) installato e attivo
	- Python exe is in Windows Path.
	- My protected Python environment is working in:
		- D:\Documents\GitHub\EmanueleTinari\Python\env\
		- In my protected Python environment ho installato i packages come indicato nella seguente tabella:

| No | Package | Version |
| --- | --- | --- |
| 1 | aiohappyeyeballs | 2.6.1 |
| 2 | aiohttp | 3.12.14 |
| 3 | aiosignal | 1.4.0 |
| 4 | aiosqlite | 0.21.0 |
| 5 | annotated-types | 0.7.0 |
| 6 | anyio | 4.9.0 |
| 7 | attrs | 25.3.0 |
| 8 | backoff | 2.2.1 |
| 9 | banks | 2.1.3 |
| 10 | bcrypt | 4.3.0 |
| 11 | beautifulsoup4 | 4.13.4 |
| 12 | build | 1.2.2.post1 |
| 13 | cachetools | 5.5.2 |
| 14 | certifi | 2025.7.14 |
| 15 | cffi | 1.17.1 |
| 16 | charset-normalizer | 3.4.2 |
| 17 | chromadb | 1.0.15 |
| 18 | click | 8.2.1 |
| 19 | colorama | 0.4.6 |
| 20 | coloredlogs | 15.0.1 |
| 21 | cryptography | 45.0.5 |
| 22 | dataclasses-json | 0.6.7 |
| 23 | defusedxml | 0.7.1 |
| 24 | Deprecated | 1.2.18 |
| 25 | dirtyjson | 1.0.8 |
| 26 | distro | 1.9.0 |
| 27 | durationpy | 0.10 |
| 28 | EbookLib | 0.19 |
| 29 | filelock | 3.18.0 |
| 30 | filetype | 1.2.0 |
| 31 | flatbuffers | 25.2.10 |
| 32 | frozenlist | 1.7.0 |
| 33 | fsspec | 2025.7.0 |
| 34 | google-auth | 2.40.3 |
| 35 | googleapis-common-protos | 1.70.0 |
| 36 | greenlet | 3.2.3 |
| 37 | griffe | 1.7.3 |
| 38 | grpcio | 1.73.1 |
| 39 | h11 | 0.16.0 |
| 40 | hf-xet | 1.1.5 |
| 41 | html2text | 2025.4.15 |
| 42 | httpcore | 1.0.9 |
| 43 | httptools | 0.6.4 |
| 44 | httpx | 0.28.1 |
| 45 | huggingface-hub | 0.33.4 |
| 46 | humanfriendly | 10.0 |
| 47 | idna | 3.10 |
| 48 | importlib_metadata | 8.7.0 |
| 49 | importlib_resources | 6.5.2 |
| 50 | Jinja2 | 3.1.6 |
| 51 | jiter | 0.10.0 |
| 52 | joblib | 1.5.1 |
| 53 | jsonpatch | 1.33 |
| 54 | jsonpointer | 3.0.0 |
| 55 | jsonschema | 4.24.1 |
| 56 | jsonschema-specifications | 2025.4.1 |
| 57 | kubernetes | 33.1.0 |
| 58 | langchain | 0.3.26 |
| 59 | langchain-core | 0.3.69 |
| 60 | langchain-text-splitters | 0.3.8 |
| 61 | langsmith | 0.4.6 |
| 62 | llama-cloud | 0.1.32 |
| 63 | llama-cloud-services | 0.6.43 |
| 64 | llama-index | 0.12.49 |
| 65 | llama-index-agent-openai | 0.4.12 |
| 66 | llama-index-cli | 0.4.4 |
| 67 | llama-index-core | 0.12.49 |
| 68 | llama-index-embeddings-huggingface | 0.5.5 |
| 69 | llama-index-embeddings-openai | 0.3.1 |
| 70 | llama-index-indices-managed-llama-cloud | 0.7.10 |
| 71 | llama-index-instrumentation | 0.3.0 |
| 72 | llama-index-llms-ollama | 0.6.2 |
| 73 | llama-index-llms-openai | 0.4.7 |
| 74 | llama-index-multi-modal-llms-openai | 0.5.3 |
| 75 | llama-index-program-openai | 0.3.2 |
| 76 | llama-index-question-gen-openai | 0.3.1 |
| 77 | llama-index-readers-file | 0.4.11 |
| 78 | llama-index-readers-llama-parse | 0.4.0 |
| 79 | llama-index-workflows | 1.1.0 |
| 80 | llama-parse | 0.6.43 |
| 81 | lxml | 6.0.0 |
| 82 | markdown-it-py | 3.0.0 |
| 83 | MarkupSafe | 3.0.2 |
| 84 | marshmallow | 3.26.1 |
| 85 | mdurl | 0.1.2 |
| 86 | mmh3 | 5.1.0 |
| 87 | mpmath | 1.3.0 |
| 88 | multidict | 6.6.3 |
| 89 | mypy_extensions | 1.1.0 |
| 90 | nest-asyncio | 1.6.0 |
| 91 | networkx | 3.5 |
| 92 | nltk | 3.9.1 |
| 93 | numpy | 2.3.1 |
| 94 | oauthlib | 3.3.1 |
| 95 | ollama | 0.5.1 |
| 96 | onnxruntime | 1.22.1 |
| 97 | openai | 1.97.0 |
| 98 | opentelemetry-api | 1.35.0 |
| 99 | opentelemetry-exporter-otlp-proto-common | 1.35.0 |
| 100 | opentelemetry-exporter-otlp-proto-grpc | 1.35.0 |
| 101 | opentelemetry-proto | 1.35.0 |
| 102 | opentelemetry-sdk | 1.35.0 |
| 103 | opentelemetry-semantic-conventions | 0.56b0 |
| 104 | orjson | 3.11.0 |
| 105 | overrides | 7.7.0 |
| 106 | packaging | 25.0 |
| 107 | pandas | 2.2.3 |
| 108 | pdfminer.six | 20250506 |
| 109 | pillow | 11.3.0 |
| 110 | pip | 25.1.1 |
| 111 | platformdirs | 4.3.8 |
| 112 | posthog | 5.4.0 |
| 113 | propcache | 0.3.2 |
| 114 | protobuf | 6.31.1 |
| 115 | pyasn1 | 0.6.1 |
| 116 | pyasn1_modules | 0.4.2 |
| 117 | pybase64 | 1.4.1 |
| 118 | pycparser | 2.22 |
| 119 | pydantic | 2.11.7 |
| 120 | pydantic_core | 2.33.2 |
| 121 | Pygments | 2.19.2 |
| 122 | pypdf | 5.8.0 |
| 123 | PyPika | 0.48.9 |
| 124 | pyproject_hooks | 1.2.0 |
| 125 | pyreadline3 | 3.5.4 |
| 126 | python-dateutil | 2.9.0.post0 |
| 127 | python-dotenv | 1.1.1 |
| 128 | pytz | 2025.2 |
| 129 | PyYAML | 6.0.2 |
| 130 | referencing | 0.36.2 |
| 131 | regex | 2024.11.6 |
| 132 | requests | 2.32.4 |
| 133 | requests-oauthlib | 2.0.0 |
| 134 | requests-toolbelt | 1.0.0 |
| 135 | rich | 14.0.0 |
| 136 | rpds-py | 0.26.0 |
| 137 | rsa | 4.9.1 |
| 138 | safetensors | 0.5.3 |
| 139 | scikit-learn | 1.7.0 |
| 140 | scipy | 1.16.0 |
| 141 | sentence-transformers | 5.0.0 |
| 142 | setuptools | 80.9.0 |
| 143 | shellingham | 1.5.4 |
| 144 | six | 1.17.0 |
| 145 | sniffio | 1.3.1 |
| 146 | soupsieve | 2.7 |
| 147 | SQLAlchemy | 2.0.41 |
| 148 | striprtf | 0.0.26 |
| 149 | sympy | 1.14.0 |
| 150 | tenacity | 9.1.2 |
| 151 | threadpoolctl | 3.6.0 |
| 152 | tiktoken | 0.9.0 |
| 153 | tokenizers | 0.21.2 |
| 154 | torch | 2.7.1 |
| 155 | tqdm | 4.67.1 |
| 156 | transformers | 4.53.2 |
| 157 | typer | 0.16.0 |
| 158 | typing_extensions | 4.14.1 |
| 159 | typing-inspect | 0.9.0 |
| 160 | typing-inspection | 0.4.1 |
| 161 | tzdata | 2025.2 |
| 162 | urllib3 | 2.5.0 |
| 163 | uvicorn | 0.35.0 |
| 164 | watchfiles | 1.1.0 |
| 165 | websocket-client | 1.8.0 |
| 166 | websockets | 15.0.1 |
| 167 | wrapt | 1.17.2 |
| 168 | yarl | 1.20.1 |
| 169 | zipp | 3.23.0 |
| 170 | zstandard | 0.23.0 |


- Agisci come un programmatore Senior
- Genera ogni riga di codice preceduta da commento in Italiano ed in Inglese come nell'esempio seguente:
	- # Riga commento in Italiano
	- # Riga commento in Inglese
	- Riga di codice
- Nome di questa chat: "webScraper"
- Genera scraper Python.
- Link di partenza indicato in var "startUrl"
- Scarica da startUrl tutto ciò che trova seguendo tutti i link di quella pagina e per N sottopagine
	- N lo metto io nella var "depth".
- Non seguire nessun link che non inizi col dominio presente in "startUrl".
- Ignora file "robots.txt"
- Cartella di salvataggio: "I:/scraper/nomesito" (no http ma solo www.sito.xxx)
- Scarica tutti i file (HTML, PDF, immagini, CSS, JS, ecc.), no max size.
- Generi una lista dei file scaricati in "I:/scraper", in HTML, file con nome: "come la cartella di destinazione + .HTML".
	- Deve ricreare l'albero esatto del sito con il link al file locale qualsiasi esso sia così si aprirà nel browser qualsiasi riga clicco sul link nell'html.
- Ad ogni esecuzione del codice successiva alla prima, se il file è già presente e identico, non riscaricarlo.
- Dopo aver completato il download di una pagina (HTML + risorse collegate), attendere 10 secondi prima della prossima pagina
- Aggiunta un "flag force" per forzare il riscaricamento anche se il file è identico.
- Scraper performante e asincrono usando asyncio + aiohttp con gestione di limite massimo di worker (concurrency) e pausa di 10 secondi per ogni worker.



OLD version:

# === IMPORTAZIONI ===
# Librerie per operazioni asincrone, file I/O, parsing URL, manipolazione HTML, ecc.
import asyncio
# Libreria per effettuare richieste HTTP asincrone
import aiohttp
import aiofiles
import hashlib
from urllib.parse import urljoin, urlparse
from pathlib import Path
from bs4 import BeautifulSoup
import re

# === CONFIGURAZIONE ===
# Dominio iniziale e da cui non uscire
startHost = "www.vatican.va"
# URL da cui partire
startUrl = "https://www.vatican.va/archive/bible/nova_vulgata/documents/nova-vulgata_index_lt.html"  # URL iniziale
# Prefisso di path consentito
allowed_path_prefix = "/archive/bible/nova_vulgata/documents/"
# Profondità massima del crawling
depth = 4
# Solo per intestazione indice e percorso base
baseUrl = "https://" + startHost
# Cartella locale dove salvare
download_folder = Path(f"I:/scraper/{startHost}")
# File HTML con indice locale
index_file = Path(f"I:/scraper/{startHost}.html")
# Pausa tra download (per worker)
wait_seconds = 10
# Numero massimo di download paralleli
concurrency = 10
# Se True forza il riscaricamento di tutti i file
force = False

# === VARIABILI GLOBALI ===
# Insieme dei link già visitati
visited = set()
# Lista dei link per l'indice finale
index_links = []
domain = startHost
# Semaforo per limitare le richieste concorrenti
semaphore = asyncio.Semaphore(concurrency)
# Supponiamo venga importato da file Python per semplicità
# === ARRAY arrVers ===
# Array importato o copiato da input utente (verrà incluso qui come struttura dati Python)
from arr_vers import arrVers

# === FUNZIONI DI SUPPORTO ===
# Funzione per sapere se un link è interno al dominio
def is_internal_link(link):
    """Verifica se il link è interno al dominio e alla sezione Nova Vulgata."""
    parsed = urlparse(link)
    return (
        (parsed.netloc == "" or parsed.netloc == domain) and
        parsed.path.startswith(allowed_path_prefix)
    )

# Funzione per normalizzare i link
def normalize_link(base, link):
    """Costruisce un link assoluto pulito da ancore."""
    return urljoin(base, link.split('#')[0])

def get_local_filename(url):
    """Genera il path locale dove salvare il contenuto dell'URL."""
    parsed = urlparse(url)
    path = parsed.path
    if path.endswith("/") or path == "":
        path += "index.html"
    elif not Path(path).suffix:
        path += ".html"
    return download_folder / parsed.netloc / path.lstrip("/")

async def file_hash(path):
    """Calcola hash SHA256 di un file locale (se esiste)."""
    h = hashlib.sha256()
    try:
        async with aiofiles.open(path, 'rb') as f:
            while True:
                chunk = await f.read(4096)
                if not chunk:
                    break
                h.update(chunk)
        return h.hexdigest()
    except FileNotFoundError:
        return None

async def save_file(url, content):
    """Salva contenuto in locale, evitando duplicati se hash coincide."""
    local_path = get_local_filename(url)
    local_path.parent.mkdir(parents=True, exist_ok=True)

    if local_path.exists() and not force:
        existing_hash = await file_hash(local_path)
        new_hash = hashlib.sha256(content if isinstance(content, bytes) else content.encode("utf-8")).hexdigest()
        if existing_hash == new_hash:
            print(f"🟡 Già presente identico: {url}")
            relative_path = local_path.relative_to(download_folder.parent)
            index_links.append((url, relative_path.as_posix()))
            return False

    mode = "wb" if isinstance(content, bytes) else "w"
    async with aiofiles.open(local_path, mode, encoding=None if mode == "wb" else "utf-8") as f:
        await f.write(content)
    print(f"✅ Salvato: {url} → {local_path}")
    relative_path = local_path.relative_to(download_folder.parent)
    index_links.append((url, relative_path.as_posix()))
    return True

async def fetch(session, url):
    """Scarica il contenuto dell'URL, restituendo tipo e contenuto."""
    async with semaphore:
        try:
            async with session.get(url) as resp:
                if resp.status != 200:
                    print(f"❌ Errore {resp.status} su {url}")
                    return None, None
                content_type = resp.headers.get("Content-Type", "")
                if "text/html" in content_type:
                    text = await resp.text()
                    return "html", text
                else:
                    data = await resp.read()
                    return "binary", data
        except Exception as e:
            print(f"⚠️ Errore su {url}: {e}")
            return None, None

async def crawl(session, url, current_depth):
    """Crawling asincrono ricorsivo, salva file e segue link interni validi."""
    if current_depth > depth or url in visited:
        return
    visited.add(url)

    tipo, content = await fetch(session, url)
    if content is None:
        return

    scritto = await save_file(url, content)
    if tipo == "html":
        soup = BeautifulSoup(content, "html.parser")
        tasks = []
        for tag in soup.find_all(["a", "img", "script", "link"]):
            href = tag.get("href") or tag.get("src")
            if not href:
                continue
            link = normalize_link(url, href)
            if is_internal_link(link) and link not in visited:
                tasks.append(crawl(session, link, current_depth + 1))
        if tasks:
            await asyncio.gather(*tasks)

    if scritto:
        print(f"⏳ Attendo {wait_seconds} secondi prima di continuare...\n")
        await asyncio.sleep(wait_seconds)

async def genera_indice_html():
    """Crea file HTML cliccabile con elenco dei file salvati localmente."""
    async with aiofiles.open(index_file, "w", encoding="utf-8") as f:
        await f.write("<!DOCTYPE html><html><head><meta charset='utf-8'><title>Indice</title></head><body>\n")
        await f.write(f"<h1>Contenuti scaricati da {startUrl}</h1>\n<ul>\n")
        for url, rel_path in sorted(index_links):
            await f.write(f"<li><a href='{rel_path}' target='_blank'>{url}</a></li>\n")
        await f.write("</ul>\n</body></html>")
    print(f"📄 Indice HTML creato: {index_file}")

async def main():
    """Avvia la sessione e lo scraping a partire da startUrl."""
    async with aiohttp.ClientSession() as session:
        await crawl(session, startUrl, 0)
    await genera_indice_html()
    print("🏁 Fine scraping.")

if __name__ == "__main__":
    asyncio.run(main())



NEW version:

# === IMPORTAZIONI ===
# Librerie per operazioni asincrone, file I/O, parsing URL, manipolazione HTML, ecc.
import asyncio
import aiofiles
import hashlib
from urllib.parse import urljoin, urlparse
from pathlib import Path
from bs4 import BeautifulSoup
import re

# === CONFIGURAZIONE ===
# Dominio iniziale
host = "www.vatican.va"
# URL da cui partire
url_iniziale = "https://www.vatican.va/archive/bible/nova_vulgata/documents/nova-vulgata_index_lt.html"
# Massima profondità di ricorsione
profondita_massima = 4
# Timeout tra le richieste
attesa_secondi = 10
# Massimo numero di richieste concorrenti
concorrenza = 10
# Forzare il riscaricamento di file già presenti
forza = False
# Cartella dove salvare i file
cartella_scaricamento = Path("I:/scraper_bibbia")
# File indice HTML da generare
file_indice = cartella_scaricamento / "indice.html"
# Estensione file finale
estensione_html = ".html"

# === VARIABILI GLOBALI ===
# Insieme dei link già visitati
visitati = set()
# Lista dei link per l'indice finale
link_indice = []
# Semaforo per limitare le richieste concorrenti
semaforo = asyncio.Semaphore(concorrenza)

# === ARRAY arrVers ===
# Array importato o copiato da input utente (verrà incluso qui come struttura dati Python)
from arr_vers import arrVers  # Supponiamo venga importato da file Python per semplicità

# === FUNZIONI DI SUPPORTO ===
# Funzione per sapere se un link è interno al dominio
def link_interno(link):
    parsed = urlparse(link)
    return parsed.netloc == "" or parsed.netloc == host

# Funzione per normalizzare i link
def normalizza_link(base, link):
    return urljoin(base, link.split('#')[0])

# Funzione per trovare il nome personalizzato del file HTML basato sull'array arrVers
def nome_personalizzato(url):
    # Estrai solo il nome del file
    nome_file = Path(urlparse(url).path).name.lower()

    # Se non è uno dei file VT o NT, restituisci None
    if not (nome_file.startswith("nova-vulgata_vt_") or nome_file.startswith("nova-vulgata_nt_")):
        return None

    # Determina se è Antico o Nuovo Testamento
    prefisso = "AT - " if "_vt_" in nome_file else "NT - "

    # Estrai chiave dopo "_vt_" o "_nt_"
    chiave = nome_file.split("_vt_" if "_vt_" in nome_file else "_nt_")[1].split("_")[0]

    # Cerca in arrVers corrispondenza con colonna 1 (Numero libro) o 11 (AbbrLT)
    indice = None
    for i, riga in enumerate(arrVers):
        # Controlla se chiave è contenuta in colonna 11 (abbrLT, es: "Gen")
        if chiave.lower() in riga[11].lower():
            indice = i
            break

    # Se non trovato, esci
    if indice is None:
        return None

    # Prendi numero libro (colonna 1), nome IT (colonna 7)
    numero = int(arrVers[indice][1])
    numero_str = f"{numero:02}"
    nome_it = arrVers[indice][7]

    # Se il nome contiene "Primo" o "Secondo" ecc., normalizza con 1/2 + nome
    if nome_it.startswith("Primo"):
        nome_it = "1 " + nome_it.replace("Primo libro di ", "").replace("Primo libro dei ", "").replace(" (=", "").split("=")[0].strip()
    elif nome_it.startswith("Secondo"):
        nome_it = "2 " + nome_it.replace("Secondo libro di ", "").replace("Secondo libro dei ", "").replace(" (=", "").split("=")[0].strip()

    return f"{prefisso}{numero_str} {nome_it}{estensione_html}"

# Funzione per determinare il percorso locale del file scaricato
def percorso_locale(url):
    nome_custom = nome_personalizzato(url)
    if nome_custom:
        return cartella_scaricamento / nome_custom
    # fallback classico se non è matchabile
    parsed = urlparse(url)
    path = parsed.path
    if path.endswith("/") or path == "":
        path += "index.html"
    elif not Path(path).suffix:
        path += ".html"
    return cartella_scaricamento / parsed.netloc / path.lstrip("/")

# Funzione per calcolare hash di un file
async def hash_file(percorso):
    h = hashlib.sha256()
    try:
        async with aiofiles.open(percorso, 'rb') as f:
            while True:
                blocco = await f.read(4096)
                if not blocco:
                    break
                h.update(blocco)
        return h.hexdigest()
    except FileNotFoundError:
        return None

# Funzione per salvare un file
async def salva_file(url, contenuto):
    percorso = percorso_locale(url)
    percorso.parent.mkdir(parents=True, exist_ok=True)

    if percorso.exists() and not forza:
        hash_esistente = await hash_file(percorso)
        hash_nuovo = hashlib.sha256(contenuto if isinstance(contenuto, bytes) else contenuto.encode("utf-8")).hexdigest()
        if hash_esistente == hash_nuovo:
            print(f"🟡 Già presente identico: {url}")
            link_indice.append((url, percorso.relative_to(cartella_scaricamento).as_posix()))
            return False

    # Scrittura file
    modo = "wb" if isinstance(contenuto, bytes) else "w"
    async with aiofiles.open(percorso, modo, encoding=None if modo == "wb" else "utf-8") as f:
        await f.write(contenuto)
    print(f"✅ Salvato: {url} → {percorso}")
    link_indice.append((url, percorso.relative_to(cartella_scaricamento).as_posix()))
    return True

# Funzione per scaricare e analizzare ricorsivamente
async def esplora(sessione, url, livello):
    if livello > profondita_massima or url in visitati:
        return
    visitati.add(url)

    async with semaforo:
        try:
            async with sessione.get(url) as risposta:
                if risposta.status != 200:
                    print(f"❌ Errore {risposta.status} su {url}")
                    return
                tipo = risposta.headers.get("Content-Type", "")
                if "text/html" in tipo:
                    testo = await risposta.text()
                    tipo_contenuto = "html"
                else:
                    testo = await risposta.read()
                    tipo_contenuto = "binario"
        except Exception as e:
            print(f"⚠️ Errore su {url}: {e}")
            return

    salvato = await salva_file(url, testo)
    if tipo_contenuto == "html":
        soup = BeautifulSoup(testo, "html.parser")
        tasks = []
        for tag in soup.find_all(["a"]):
            href = tag.get("href")
            if not href:
                continue
            link = normalizza_link(url, href)
            if link_interno(link):
                tasks.append(esplora(sessione, link, livello + 1))
        if tasks:
            await asyncio.gather(*tasks)

    if salvato:
        print(f"⏳ Attendo {attesa_secondi} secondi...")
        await asyncio.sleep(attesa_secondi)

# Funzione per generare l'indice finale
async def genera_indice():
    async with aiofiles.open(file_indice, "w", encoding="utf-8") as f:
        await f.write("<!DOCTYPE html><html><head><meta charset='utf-8'><title>Indice</title></head><body>\n")
        await f.write(f"<h1>Indice contenuti scaricati</h1>\n<ul>\n")
        for url, relativo in sorted(link_indice):
            await f.write(f"<li><a href='{relativo}' target='_blank'>{url}</a></li>\n")
        await f.write("</ul></body></html>")
    print(f"📄 Indice generato: {file_indice}")

# Funzione principale
async def main():
    async with aiohttp.ClientSession() as sessione:
        await esplora(sessione, url_iniziale, 0)
    await genera_indice()
    print("🏁 Completato.")

# Esecuzione se avviato come script
if __name__ == "__main__":
    asyncio.run(main())



Invio script arr_vers:

# === arrVers.py ===
# Legenda colonne completa:
# - Colonna 1: Numero libro
# - Colonna 2: Testamento
# - Colonna 3: Gruppo
# - Colonna 4: Nome libro IT
# - Colonna 5: AbbrIT
# - Colonna 6: Nome libro LT
# - Colonna 7: AbbrLT
# - Colonna 8: Nome libro EN
# - Colonna 9: AbbrEN

Invio spezzone dell'array "arrVers":

# === arrVers.py ===
# Legenda colonne completa:
# - Colonna 1: Numero libro
# - Colonna 2: Testamento
# - Colonna 3: Gruppo
# - Colonna 4: Nome libro IT
# - Colonna 5: AbbrIT
# - Colonna 6: Nome libro LT
# - Colonna 7: AbbrLT
# - Colonna 8: Nome libro EN
# - Colonna 9: AbbrEN

arrVers = [
    ['Numero libro','1', 'Testamento','Antico Testamento', 'Gruppo','Il Pentateuco', 'Nome libro IT','Genesi','AbbrIT','Gn;Gen;Ge','Nome libro LT','Liber Genesis','AbbrLT','Gen','Nome libro EN','Genesis','AbbrEN','Gen'],
    ['Numero libro','2', 'Testamento','Antico Testamento', 'Gruppo','Il Pentateuco', 'Nome libro IT','Esodo','AbbrIT','Es;Eso;Eo','Nome libro LT','Liber Exodus','AbbrLT','Ex','Nome libro EN','Exodus','AbbrEN','Exod'],
    ['Numero libro','3', 'Testamento','Antico Testamento', 'Gruppo','Il Pentateuco', 'Nome libro IT','Levitico','AbbrIT','Lv;Le','Nome libro LT','Liber Leviticus','AbbrLT','Lev','Nome libro EN','Leviticus','AbbrEN','Lev'],
    ['Numero libro','4', 'Testamento','Antico Testamento', 'Gruppo','Il Pentateuco', 'Nome libro IT','Numeri','AbbrIT','Nm;NÙ,'Nome libro LT','Liber Numeri','AbbrLT','Num','Nome libro EN','Numbers','AbbrEN','Nun'],
    ['Numero libro','5', 'Testamento','Antico Testamento', 'Gruppo','Il Pentateuco', 'Nome libro IT','Deuteronomio','AbbrIT','Dt;Deut;De','Nome libro LT','Liber Deuteronomii','AbbrLT','Deut','Nome libro EN','Deuteronomy','AbbrEN','Deut'],

L'array inviato non è completo né come puoi vedere, chiuso. L'import lo richiama totalmente, è per farti capire come è strutturato e dove trovare i dati richiesti.

Il tuo compito:
- Riscrivi completamente il codice dello scrip fondendo i due incollati (OLD e NEW) in uno solo con:
	- Commenti doppia lingua come indicato sopra;
	- Nomi Variabili prese da OLD;
	- Controlla e dimmi come potrebbe essere migliorato
	- Non togliere nulla!
	- Lascia tutti i commenti come sono, anzi aggiungine! ma non togliere!
	- Lasciare nome variabili così come sono!
	- Lasciare l'ordine delle funzioni così come sono, modificalo solo se necessario